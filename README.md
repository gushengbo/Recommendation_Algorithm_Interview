1. LR线性回归的原理和推导
2. XGBoost原理及其推导
  是一种基于梯度提升决策树（Gradient Boosting Decision Trees, GBDT）的高效实现。GBDT是一种集成学习方法，它通过逐步构建多个决策树，每棵树都是在前一棵树的基础上进行改进。具体来说，GBDT使用梯度下降的思想来最小化损失函数，逐步调整模型的预测值。

GBDT的基本公式为：![image](https://github.com/user-attachments/assets/4eb759a1-5172-45f8-8352-8f848a087bd2) 每一个f(x)目标是通过每棵树的学习来减少上一棵树的误差



3. 生成式检索相比于传统检索的优势。基于大模型，可以学习到文本潜在的高级语义特征。达到更个性化的检索、推荐。
4. 生成式检索baseline是unimo-text-1.0-large
5. 每个广告都包含落地页特征文本和核心词特征文本。训练集是搜索词对应的广告id。所以我们直观想法就是，得让模型先充分学习所有广告的文本信息，然后再去训练训练集。落地页跟核心词都是一长文本，我们截取成短文本，以匹配搜索词的长度。落地页特征是比较粗糙的特征，每一句话跟广告不一定完全匹配，比如（新能源汽车的广告，落地页特征可能是，xxx汽车店，4S店。），而且会有几条非常相似的广告，对应的落地页特征完全一样。核心词特征也是类似，但会比落地页特征更准确一些。多阶段学习相当于让模型粗略地记忆所有广告的特征，然后再通过训练集去更加精细地记忆广告。
6. DPR内部结构是两个BERT模型，六层transformer的encoder（是什么？） BM25是稀疏检索模型，通过关键词匹配。
7. 损失函数，先softmax, 再交叉熵。采用余弦相似度，因为可以避免向量长度的影响。
8. TSNE是什么？
9. 使用Neural-Chat做数据增强，因为问题只有很简短的一句话，去匹配论文的摘要。用大语言模型补充更多的信息，去匹配长度相似的论文摘要。
10. 迭代伪标签，充分利用无标签数据集。
   
